{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lead_lag_normalized import add_normalizations, event_study_value # Assuming lead_lag_normalized.py is in 'code'\n",
        "import matplotlib.pyplot as plt\n",
        "from lead_lag_analysis import xcorr_by_company, plot_xcorr, event_study, plot_event\n",
        "from lead_lag_normalized import event_study_value # Import event_study_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "REPO_ROOT = \"/Users/beszabo/bene/szakdolgozat\"\n",
        "DERIVED_DIR = os.path.join(REPO_ROOT, \"data\", \"panels\")\n",
        "FIG_DIR = os.path.join(REPO_ROOT, \"figures\")\n",
        "\n",
        "# Use balanced, calendar-complete panel for event study\n",
        "PANEL_CSV = os.path.join(DERIVED_DIR, \"company_weekly_panel_analysis_ready.csv\")\n",
        "\n",
        "panel = pd.read_csv(PANEL_CSV, parse_dates=['week_start'])\n",
        "\n",
        "# Map balanced panel column names back to legacy names used in downstream code\n",
        "alias_map = {\n",
        "    'nyt_pos_share': 'mean_pos',\n",
        "    'nyt_neg_share': 'mean_neg',\n",
        "    'nyt_sentiment': 'sentiment_score',\n",
        "    'nyt_non_neutral_share': 'non_neutral_share',\n",
        "}\n",
        "for src, dst in alias_map.items():\n",
        "    if src in panel.columns and dst not in panel.columns:\n",
        "        panel[dst] = panel[src]\n",
        "# Alias for old 'num_articles' usages\n",
        "if 'num_articles' not in panel.columns and 'NYT_mention' in panel.columns:\n",
        "    panel['num_articles'] = panel['NYT_mention']\n",
        "\n",
        "# Treat truly unobserved meme weeks as missing (NaN) for outcome averaging\n",
        "if 'num_memes' in panel.columns:\n",
        "    if 'mean_meme_sentiment' in panel.columns:\n",
        "        missing_memes = (panel['num_memes'] == 0) & (panel['mean_meme_sentiment'].isna())\n",
        "    else:\n",
        "        missing_memes = (panel['num_memes'] == 0)\n",
        "    panel['num_memes_na'] = panel['num_memes'].astype(float)\n",
        "    panel.loc[missing_memes, 'num_memes_na'] = np.nan\n",
        "    # NaN-aware z-score per company (computed on observed weeks only)\n",
        "    stats = (panel.loc[~missing_memes]\n",
        "                  .groupby('company')['num_memes']\n",
        "                  .agg(mu='mean', sd='std')\n",
        "                  .reset_index())\n",
        "    panel = panel.merge(stats, on='company', how='left')\n",
        "    panel['sd'] = panel['sd'].replace(0, np.nan)\n",
        "    panel['num_memes_z_es'] = (panel['num_memes_na'] - panel['mu']) / panel['sd']\n",
        "    # For compatibility with plotting/xcorr helpers\n",
        "    panel['num_memes_z'] = panel['num_memes_z_es']\n",
        "\n",
        "# Event study on NaN-aware z outcome, using balanced panel indexing\n",
        "pos_z, neg_z = event_study_value(panel,\n",
        "                                 pos_feature='mean_pos',\n",
        "                                 neg_feature='mean_neg',\n",
        "                                 value_col='num_memes_z_es',\n",
        "                                 window=3)\n",
        "\n",
        "os.makedirs(FIG_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def _sanitize_filename(name: str) -> str:\n",
        "    return re.sub(r\"[^A-Za-z0-9_-]+\", \"_\", str(name)).strip(\"_\")\n",
        "\n",
        "# plot event study with 95% CI\n",
        "def plot_event_ci(df: pd.DataFrame, value_col: str, title: str, out_path: str):\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "    agg = df.groupby('tau')[value_col].agg(['mean', 'std', 'count']).reset_index()\n",
        "    agg['se'] = agg['std'] / np.sqrt(agg['count'].clip(lower=1))\n",
        "    agg['lo'] = agg['mean'] - 1.96 * agg['se']\n",
        "    agg['hi'] = agg['mean'] + 1.96 * agg['se']\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(agg['tau'], agg['mean'], marker='o', label='Mean')\n",
        "    plt.fill_between(agg['tau'], agg['lo'], agg['hi'], alpha=0.2, label='95% CI')\n",
        "    plt.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
        "    plt.xlabel('Weeks around event')\n",
        "    plt.ylabel(f'Mean {value_col}')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='best', frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "# draw simple brand-level time-series\n",
        "def plot_brand_timeseries(panel: pd.DataFrame,\n",
        "                          company: str,\n",
        "                          left_col: str = 'num_articles',\n",
        "                          right_col: str = 'num_memes_z',\n",
        "                          smooth: int = 0,\n",
        "                          out_path: str | None = None):\n",
        "    g = panel.loc[panel['company'] == company].sort_values('week_start').copy()\n",
        "    if g.empty:\n",
        "        return\n",
        "\n",
        "    x = g['week_start']\n",
        "    left = g[left_col].astype(float)\n",
        "    right = g[right_col].astype(float)\n",
        "\n",
        "    if smooth and smooth > 1:\n",
        "        left = left.rolling(window=smooth, min_periods=1).mean()\n",
        "        right = right.rolling(window=smooth, min_periods=1).mean()\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(9, 3.5))\n",
        "    color_left, color_right = '#1f77b4', '#d62728'\n",
        "\n",
        "    ax1.plot(x, left, color=color_left, label=left_col)\n",
        "    ax1.set_xlabel('Week')\n",
        "    ax1.set_ylabel(left_col, color=color_left)\n",
        "    ax1.tick_params(axis='y', labelcolor=color_left)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(x, right, color=color_right, label=right_col, alpha=0.85)\n",
        "    ax2.set_ylabel(right_col, color=color_right)\n",
        "    ax2.tick_params(axis='y', labelcolor=color_right)\n",
        "\n",
        "    fig.suptitle(f'{company}: {left_col} vs {right_col}')\n",
        "    fig.tight_layout()\n",
        "\n",
        "    if out_path:\n",
        "        plt.savefig(out_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved /Users/beszabo/bene/szakdolgozat/figures/ts_facebook_num_articles_vs_num_memes_z.png\n",
            "Saved /Users/beszabo/bene/szakdolgozat/figures/ts_youtube_num_articles_vs_num_memes_z.png\n",
            "Saved /Users/beszabo/bene/szakdolgozat/figures/ts_netflix_num_articles_vs_num_memes_z.png\n",
            "Saved /Users/beszabo/bene/szakdolgozat/figures/ts_google_num_articles_vs_num_memes_z.png\n",
            "Saved /Users/beszabo/bene/szakdolgozat/figures/ts_instagram_num_articles_vs_num_memes_z.png\n",
            "Saved /Users/beszabo/bene/szakdolgozat/figures/ts_apple_num_articles_vs_num_memes_z.png\n",
            "Saved /Users/beszabo/bene/szakdolgozat/figures/ts_microsoft_num_articles_vs_num_memes_z.png\n",
            "Saved /Users/beszabo/bene/szakdolgozat/figures/ts_amazon_num_articles_vs_num_memes_z.png\n",
            "Saved /Users/beszabo/bene/szakdolgozat/figures/ts_tesla_num_articles_vs_num_memes_z.png\n",
            "Saved /Users/beszabo/bene/szakdolgozat/figures/ts_spotify_num_articles_vs_num_memes_z.png\n",
            "Saved CI event-study plots.\n"
          ]
        }
      ],
      "source": [
        "# Descriptive brand time-series and event-study with 95% CIs\n",
        "\n",
        "# Select a few brands (top by NYT articles)\n",
        "top_companies = (\n",
        "    panel.groupby('company')['NYT_mention']\n",
        "            .sum()\n",
        "            .sort_values(ascending=False)\n",
        "            .head(10)\n",
        "            .index\n",
        "            .tolist()\n",
        ")\n",
        "\n",
        "for c in top_companies:\n",
        "    out = os.path.join(FIG_DIR, f\"ts_{_sanitize_filename(c)}_num_articles_vs_num_memes_z.png\")\n",
        "    plot_brand_timeseries(panel, c, left_col='NYT_mention', right_col='num_memes_z', smooth=4, out_path=out)\n",
        "    print(f\"Saved {out}\")\n",
        "\n",
        "# Event-study plots with 95% confidence intervals (NaN-aware z outcome)\n",
        "plot_event_ci(pos_z, 'num_memes_z_es',\n",
        "              'Event: Positive news vs normalized (z) meme volume (95% CI)',\n",
        "              os.path.join(FIG_DIR, 'event_pos_num_memes_z_ci.png'))\n",
        "plot_event_ci(neg_z, 'num_memes_z_es',\n",
        "              'Event: Negative news vs normalized (z) meme volume (95% CI)',\n",
        "              os.path.join(FIG_DIR, 'event_neg_num_memes_z_ci.png'))\n",
        "print('Saved CI event-study plots.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns present: ['NYT_mention', 'NYT_mention_L1', 'NYT_mention_L2', 'NYT_mention_L3', 'NYT_mention_L4', 'company', 'iso_week', 'iso_year', 'log1p_meme_engagement', 'log1p_meme_volume', 'mean_meme_sentiment', 'mean_meme_sentiment_L1', 'mean_meme_sentiment_L2', 'mean_meme_sentiment_L3', 'mean_meme_sentiment_L4', 'mean_neg', 'mean_pos', 'meme_engagement', 'meme_engagement_L1', 'meme_engagement_L2'] ...\n",
            "Date range: 2022-12-26 00:00:00 → 2024-12-30 00:00:00\n",
            "Non-null share company: 1.000\n",
            "Non-null share week_start: 1.000\n",
            "Non-null share NYT_mention: 1.000\n",
            "Non-null share num_memes: 1.000\n",
            "Non-null share meme_engagement: 0.110\n",
            "Non-null share sentiment_score: 0.443\n",
            "Non-null share mean_pos: 0.443\n",
            "Non-null share mean_neg: 0.443\n",
            "Non-null share non_neutral_share: 0.443\n",
            "\n",
            "Example continuity (top 5 companies by mentions):\n",
            "year       2022  2023  2024\n",
            "company                    \n",
            "facebook    1.0   1.0  0.09\n",
            "google      1.0   1.0  0.09\n",
            "instagram   1.0   1.0  0.09\n",
            "netflix     1.0   1.0  1.00\n",
            "youtube     1.0   1.0  1.00\n"
          ]
        }
      ],
      "source": [
        "# Sanity checks: schema, missingness, continuity\n",
        "\n",
        "required = ['company','week_start','NYT_mention','num_memes']\n",
        "optional = ['meme_engagement','sentiment_score','mean_pos','mean_neg','non_neutral_share']\n",
        "print('Columns present:', sorted(panel.columns.tolist())[:20], '...')\n",
        "missing_required = [c for c in required if c not in panel.columns]\n",
        "assert not missing_required, f\"Missing required cols: {missing_required}\"\n",
        "\n",
        "# Basic coverage\n",
        "print('Date range:', panel['week_start'].min(), '→', panel['week_start'].max())\n",
        "for c in required + optional:\n",
        "    if c in panel.columns:\n",
        "        nn = panel[c].notna().mean()\n",
        "        print(f\"Non-null share {c}: {nn:.3f}\")\n",
        "\n",
        "# NYT_mention continuity: share of weeks with >0 mentions by year\n",
        "panel['year'] = panel['week_start'].dt.year\n",
        "nyt_by_year = (panel.groupby(['company','year'])['NYT_mention']\n",
        "                    .apply(lambda s: (s.fillna(0) > 0).mean())\n",
        "                    .rename('share_weeks_with_mentions')\n",
        "                    .reset_index())\n",
        "print('\\nExample continuity (top 5 companies by mentions):')\n",
        "top5 = (panel.groupby('company')['NYT_mention'].sum().sort_values(ascending=False).head(5).index)\n",
        "print(nyt_by_year[nyt_by_year['company'].isin(top5)].pivot(index='company', columns='year', values='share_weeks_with_mentions').fillna(0.0).round(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved alternative outcome CI event-study plots.\n"
          ]
        }
      ],
      "source": [
        "# Sanity checks: alternative outcomes (num_memes_rel, log1p metrics)\n",
        "\n",
        "# Ensure alt outcomes exist\n",
        "panel['log1p_meme_volume'] = np.log1p(panel['num_memes'].fillna(0))\n",
        "if 'meme_engagement' in panel.columns:\n",
        "    panel['log1p_meme_engagement'] = np.log1p(panel['meme_engagement'].fillna(0))\n",
        "\n",
        "# num_memes_rel is provided by add_normalizations(panel)\n",
        "pos_rel, neg_rel = event_study_value(panel, pos_feature='mean_pos', neg_feature='mean_neg', value_col='num_memes_rel', window=3)\n",
        "plot_event_ci(pos_rel, 'num_memes_rel',\n",
        "              'Event: Positive news vs relative meme volume (95% CI)',\n",
        "              os.path.join(FIG_DIR, 'event_pos_num_memes_rel_ci.png'))\n",
        "plot_event_ci(neg_rel, 'num_memes_rel',\n",
        "              'Event: Negative news vs relative meme volume (95% CI)',\n",
        "              os.path.join(FIG_DIR, 'event_neg_num_memes_rel_ci.png'))\n",
        "\n",
        "# log1p outcomes\n",
        "pos_logv, neg_logv = event_study_value(panel, pos_feature='mean_pos', neg_feature='mean_neg', value_col='log1p_meme_volume', window=3)\n",
        "plot_event_ci(pos_logv, 'log1p_meme_volume',\n",
        "              'Event: Positive news vs log1p meme volume (95% CI)',\n",
        "              os.path.join(FIG_DIR, 'event_pos_log1p_meme_volume_ci.png'))\n",
        "plot_event_ci(neg_logv, 'log1p_meme_volume',\n",
        "              'Event: Negative news vs log1p meme volume (95% CI)',\n",
        "              os.path.join(FIG_DIR, 'event_neg_log1p_meme_volume_ci.png'))\n",
        "\n",
        "if 'log1p_meme_engagement' in panel.columns:\n",
        "    pos_loge, neg_loge = event_study_value(panel, pos_feature='mean_pos', neg_feature='mean_neg', value_col='log1p_meme_engagement', window=3)\n",
        "    plot_event_ci(pos_loge, 'log1p_meme_engagement',\n",
        "                  'Event: Positive news vs log1p meme engagement (95% CI)',\n",
        "                  os.path.join(FIG_DIR, 'event_pos_log1p_meme_engagement_ci.png'))\n",
        "    plot_event_ci(neg_loge, 'log1p_meme_engagement',\n",
        "                  'Event: Negative news vs log1p meme engagement (95% CI)',\n",
        "                  os.path.join(FIG_DIR, 'event_neg_log1p_meme_engagement_ci.png'))\n",
        "\n",
        "print('Saved alternative outcome CI event-study plots.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# No overlap tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Events kept (pos/neg): 350 341\n",
            "shift_m1 tau0 mean= 0.11333637073035413\n",
            "shift_p1 tau0 mean= -0.06032481282113015\n",
            "Saved no-overlap, shift tests, and demeaned-week diagnostics.\n"
          ]
        }
      ],
      "source": [
        "# Diagnostics: event selection, overlap filtering, alignment tests, week-demeaned outcomes\n",
        "\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "\n",
        "def find_events(g: pd.DataFrame, feature: str, q: float, window: int) -> List[int]:\n",
        "    \"\"\"Return indices i in g (sorted by week_start) where feature >= q-quantile.\n",
        "    Does not enforce non-overlap here.\"\"\"\n",
        "    thresh = g[feature].quantile(q)\n",
        "    return [i for i in range(len(g)) if pd.notna(g.loc[i, feature]) and g.loc[i, feature] >= thresh]\n",
        "\n",
        "\n",
        "def enforce_non_overlap(event_idx: List[int], min_gap: int) -> List[int]:\n",
        "    kept = []\n",
        "    last = -10_000\n",
        "    for i in sorted(event_idx):\n",
        "        if i - last > min_gap:\n",
        "            kept.append(i)\n",
        "            last = i\n",
        "    return kept\n",
        "\n",
        "\n",
        "def event_study_from_indices(panel: pd.DataFrame, events: Dict[str, List[int]], value_col: str, window: int) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for company, g in panel.groupby('company'):\n",
        "        g = g.sort_values('week_start').reset_index(drop=True)\n",
        "        idxs = events.get(company, [])\n",
        "        n = len(g)\n",
        "        for i in idxs:\n",
        "            for tau in range(-window, window+1):\n",
        "                j = i + tau\n",
        "                if 0 <= j < n and pd.notna(g.loc[j, value_col]):\n",
        "                    rows.append({'company': company, 'tau': tau, value_col: float(g.loc[j, value_col])})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def build_event_dict(panel: pd.DataFrame, feature: str, q: float, window: int, non_overlapping: bool, shift: int = 0) -> Dict[str, List[int]]:\n",
        "    out: Dict[str, List[int]] = {}\n",
        "    for company, g in panel.groupby('company'):\n",
        "        g = g.sort_values('week_start').reset_index(drop=True)\n",
        "        idxs = find_events(g, feature, q, window)\n",
        "        if shift:\n",
        "            idxs = [i + shift for i in idxs if 0 <= i + shift < len(g)]\n",
        "        if non_overlapping:\n",
        "            idxs = enforce_non_overlap(idxs, min_gap=window)\n",
        "        out[company] = idxs\n",
        "    return out\n",
        "\n",
        "\n",
        "# 1) Non-overlapping event windows\n",
        "win = 3\n",
        "pos_events_noov = build_event_dict(panel, 'mean_pos', q=0.90, window=win, non_overlapping=True)\n",
        "neg_events_noov = build_event_dict(panel, 'mean_neg', q=0.90, window=win, non_overlapping=True)\n",
        "\n",
        "pos_noov = event_study_from_indices(panel, pos_events_noov, 'num_memes_z', window=win)\n",
        "neg_noov = event_study_from_indices(panel, neg_events_noov, 'num_memes_z', window=win)\n",
        "\n",
        "plot_event_ci(pos_noov, 'num_memes_z', 'Event (no-overlap): Positive news vs num_memes_z', os.path.join(FIG_DIR, 'event_pos_num_memes_z_ci_nooverlap.png'))\n",
        "plot_event_ci(neg_noov, 'num_memes_z', 'Event (no-overlap): Negative news vs num_memes_z', os.path.join(FIG_DIR, 'event_neg_num_memes_z_ci_nooverlap.png'))\n",
        "\n",
        "print('Events kept (pos/neg):', sum(len(v) for v in pos_events_noov.values()), sum(len(v) for v in neg_events_noov.values()))\n",
        "\n",
        "# 2) Alignment shift tests (shift events by -1 and +1 week)\n",
        "pos_events_m1 = build_event_dict(panel, 'mean_pos', q=0.90, window=win, non_overlapping=True, shift=-1)\n",
        "pos_events_p1 = build_event_dict(panel, 'mean_pos', q=0.90, window=win, non_overlapping=True, shift=+1)\n",
        "\n",
        "for lab, ev in [('shift_m1', pos_events_m1), ('shift_p1', pos_events_p1)]:\n",
        "    dfv = event_study_from_indices(panel, ev, 'num_memes_z', window=win)\n",
        "    out = os.path.join(FIG_DIR, f'event_pos_num_memes_z_ci_{lab}.png')\n",
        "    plot_event_ci(dfv, 'num_memes_z', f'Event (pos, {lab}): num_memes_z', out)\n",
        "    # quick stat: mean at tau==0\n",
        "    if not dfv.empty:\n",
        "        print(lab, 'tau0 mean=', dfv[dfv['tau']==0]['num_memes_z'].mean())\n",
        "\n",
        "# 3) Week-demeaned outcomes\n",
        "wk_mean = panel.groupby('week_start')['num_memes_z'].transform('mean')\n",
        "panel['num_memes_z_dm'] = panel['num_memes_z'] - wk_mean\n",
        "pos_dm = event_study_from_indices(panel, pos_events_noov, 'num_memes_z_dm', window=win)\n",
        "neg_dm = event_study_from_indices(panel, neg_events_noov, 'num_memes_z_dm', window=win)\n",
        "plot_event_ci(pos_dm, 'num_memes_z_dm', 'Event (demeaned): Positive news vs num_memes_z', os.path.join(FIG_DIR, 'event_pos_num_memes_z_demeaned_ci.png'))\n",
        "plot_event_ci(neg_dm, 'num_memes_z_dm', 'Event (demeaned): Negative news vs num_memes_z', os.path.join(FIG_DIR, 'event_neg_num_memes_z_demeaned_ci.png'))\n",
        "\n",
        "print('Saved no-overlap, shift tests, and demeaned-week diagnostics.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved NYT_mention spike-based event-study plots (overall and tone-split).\n"
          ]
        }
      ],
      "source": [
        "# Diagnostics: use NYT_mention spikes as events, split by tone at event week\n",
        "\n",
        "win = 3\n",
        "# Event definition: top 10% NYT_mention per company, non-overlapping\n",
        "mention_events = build_event_dict(panel, 'NYT_mention', q=0.90, window=win, non_overlapping=True)\n",
        "\n",
        "# Unconditional on tone\n",
        "m_ev = event_study_from_indices(panel, mention_events, 'num_memes_z', window=win)\n",
        "plot_event_ci(m_ev, 'num_memes_z', 'Event (mentions spikes): num_memes_z', os.path.join(FIG_DIR, 'event_mentions_num_memes_z_ci.png'))\n",
        "\n",
        "# Split by tone sign at event week (sentiment_score >= 0 vs < 0)\n",
        "pos_split: Dict[str, List[int]] = {}\n",
        "neg_split: Dict[str, List[int]] = {}\n",
        "for company, g in panel.groupby('company'):\n",
        "    g = g.sort_values('week_start').reset_index(drop=True)\n",
        "    idxs = mention_events.get(company, [])\n",
        "    pos_idx = [i for i in idxs if pd.notna(g.loc[i, 'sentiment_score']) and g.loc[i, 'sentiment_score'] >= 0]\n",
        "    neg_idx = [i for i in idxs if pd.notna(g.loc[i, 'sentiment_score']) and g.loc[i, 'sentiment_score'] < 0]\n",
        "    pos_split[company] = pos_idx\n",
        "    neg_split[company] = neg_idx\n",
        "\n",
        "m_pos = event_study_from_indices(panel, pos_split, 'num_memes_z', window=win)\n",
        "m_neg = event_study_from_indices(panel, neg_split, 'num_memes_z', window=win)\n",
        "plot_event_ci(m_pos, 'num_memes_z', 'Event (mentions spikes, pos tone): num_memes_z', os.path.join(FIG_DIR, 'event_mentions_pos_num_memes_z_ci.png'))\n",
        "plot_event_ci(m_neg, 'num_memes_z', 'Event (mentions spikes, neg tone): num_memes_z', os.path.join(FIG_DIR, 'event_mentions_neg_num_memes_z_ci.png'))\n",
        "\n",
        "print('Saved NYT_mention spike-based event-study plots (overall and tone-split).')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved /Users/beszabo/bene/szakdolgozat/figures/xcorr_NYT_mention_vs_num_memes_z.png\n"
          ]
        }
      ],
      "source": [
        "# Sanity checks: create lags and updated CCF using NYT_mention\n",
        "\n",
        "# Create L1..L4 for selected predictors (if not already present)\n",
        "lag_sources = [c for c in ['NYT_mention','sentiment_score','mean_pos','mean_neg','non_neutral_share'] if c in panel.columns]\n",
        "panel = panel.sort_values(['company','week_start']).copy()\n",
        "for c in lag_sources:\n",
        "    for k in range(1, 5):\n",
        "        col = f\"{c}_L{k}\"\n",
        "        if col not in panel.columns:\n",
        "            panel[col] = panel.groupby('company')[c].shift(k)\n",
        "\n",
        "# Recompute CCF for NYT_mention specifically (vs num_memes_z)\n",
        "df_ccf = xcorr_by_company(panel, 'NYT_mention', max_lag=4)\n",
        "out_ccf = os.path.join(FIG_DIR, 'xcorr_NYT_mention_vs_num_memes_z.png')\n",
        "plot_xcorr(df_ccf, 'Lead-Lag: NYT_mention vs num_memes_z', out_ccf)\n",
        "print('Saved', out_ccf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Placebo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Placebo pos tau0 mean = -0.2528264676987362\n",
            "Placebo neg tau0 mean = -0.06824508001727238\n"
          ]
        }
      ],
      "source": [
        "# Placebo test: match on NYT_mention deciles within company\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# Helper: compute deciles within company using rank to avoid ties issues\n",
        "def _company_deciles(s: pd.Series, q: int = 10) -> pd.Series:\n",
        "    r = s.rank(method='first')\n",
        "    try:\n",
        "        return pd.qcut(r, q, labels=False, duplicates='drop')\n",
        "    except Exception:\n",
        "        # Fallback to single bin if not enough unique values\n",
        "        return pd.Series(0, index=s.index)\n",
        "\n",
        "# Build event indices dict (non-overlapping) using legacy feature names (aliased earlier)\n",
        "win = 3\n",
        "pos_events = {}\n",
        "neg_events = {}\n",
        "for company, g in panel.groupby('company'):\n",
        "    g = g.sort_values('week_start').reset_index(drop=True)\n",
        "    if g.empty:\n",
        "        pos_events[company] = []\n",
        "        neg_events[company] = []\n",
        "        continue\n",
        "    pos_thr = g['mean_pos'].quantile(0.90) if 'mean_pos' in g.columns else np.nan\n",
        "    neg_thr = g['mean_neg'].quantile(0.90) if 'mean_neg' in g.columns else np.nan\n",
        "    pos_idx = [i for i in range(len(g)) if pd.notna(g.loc[i,'mean_pos']) and g.loc[i,'mean_pos'] >= pos_thr]\n",
        "    neg_idx = [i for i in range(len(g)) if pd.notna(g.loc[i,'mean_neg']) and g.loc[i,'mean_neg'] >= neg_thr]\n",
        "    # enforce non-overlap (min gap == window)\n",
        "    def _keep_no_overlap(idxs):\n",
        "        kept, last = [], -10_000\n",
        "        for i in sorted(idxs):\n",
        "            if i - last > win:\n",
        "                kept.append(i); last = i\n",
        "        return kept\n",
        "    pos_events[company] = _keep_no_overlap(pos_idx)\n",
        "    neg_events[company] = _keep_no_overlap(neg_idx)\n",
        "\n",
        "# Precompute deciles per company on NYT_mention\n",
        "panel = panel.sort_values(['company','week_start']).reset_index(drop=True)\n",
        "panel['nyt_decile'] = panel.groupby('company')['NYT_mention'].transform(_company_deciles)\n",
        "\n",
        "# Placebo event-study builder\n",
        "\n",
        "def placebo_from_events(panel: pd.DataFrame, events: dict, value_col: str, window: int) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for company, g in panel.groupby('company'):\n",
        "        g = g.sort_values('week_start').reset_index(drop=True)\n",
        "        n = len(g)\n",
        "        if n == 0:\n",
        "            continue\n",
        "        dec = g['nyt_decile'] if 'nyt_decile' in g.columns else pd.Series(0, index=g.index)\n",
        "        idxs = events.get(company, [])\n",
        "        for i in idxs:\n",
        "            d = int(dec.iloc[i]) if pd.notna(dec.iloc[i]) else 0\n",
        "            candidates = [j for j in range(n) if (j != i) and (abs(j - i) > window) and (int(dec.iloc[j]) == d if pd.notna(dec.iloc[j]) else False)]\n",
        "            if not candidates:\n",
        "                candidates = [j for j in range(n) if abs(j - i) > window]\n",
        "            if not candidates:\n",
        "                continue\n",
        "            p = int(rng.choice(candidates))\n",
        "            for tau in range(-window, window+1):\n",
        "                j = p + tau\n",
        "                if 0 <= j < n and pd.notna(g.loc[j, value_col]):\n",
        "                    rows.append({'company': company, 'tau': tau, value_col: float(g.loc[j, value_col])})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Build placebo windows matched on NYT_mention deciles\n",
        "pos_placebo = placebo_from_events(panel, pos_events, value_col='num_memes_z_es', window=win)\n",
        "neg_placebo = placebo_from_events(panel, neg_events, value_col='num_memes_z_es', window=win)\n",
        "\n",
        "# Plot and quick comparison at tau=0\n",
        "plot_event_ci(pos_placebo, 'num_memes_z_es', 'Placebo (pos events, matched by NYT decile)', os.path.join(FIG_DIR, 'event_pos_num_memes_z_placebo_ci.png'))\n",
        "plot_event_ci(neg_placebo, 'num_memes_z_es', 'Placebo (neg events, matched by NYT decile)', os.path.join(FIG_DIR, 'event_neg_num_memes_z_placebo_ci.png'))\n",
        "\n",
        "if not pos_placebo.empty:\n",
        "    print('Placebo pos tau0 mean =', float(pos_placebo[pos_placebo['tau']==0]['num_memes_z_es'].mean()))\n",
        "if not neg_placebo.empty:\n",
        "    print('Placebo neg tau0 mean =', float(neg_placebo[neg_placebo['tau']==0]['num_memes_z_es'].mean()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
